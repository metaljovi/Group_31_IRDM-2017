{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning Test\n",
    "The idea behind this came as David recognised that the relevance scores given in the train.csv file were all values of numbers between 3 and 9 divided by 3.\n",
    "\n",
    "I tested this theory out by picking a pre-existing submission file from a Boosted Trees model and binning a certain range of values to specific relevance scores as in the train.csv file.\n",
    "\n",
    "Testing showed that there was little difference, and infact the binned submission seemed to have performed worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import graphlab as gl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/ssong/Downloads/IRDM/DT_submission.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/ssong/Downloads/IRDM/DT_submission.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.084597 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.084597 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[int,float]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/ssong/Downloads/IRDM/DT_submission.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/ssong/Downloads/IRDM/DT_submission.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 166693 lines in 0.063584 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 166693 lines in 0.063584 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/ssong/Downloads/IRDM/FR_submission.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/ssong/Downloads/IRDM/FR_submission.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.067432 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.067432 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[int,float]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/ssong/Downloads/IRDM/FR_submission.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/ssong/Downloads/IRDM/FR_submission.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 166693 lines in 0.074439 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 166693 lines in 0.074439 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_dt = gl.SFrame('/Users/ssong/Downloads/IRDM/DT_submission.csv')\n",
    "df_fr = gl.SFrame('/Users/ssong/Downloads/IRDM/FR_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only run 1 at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = df_dt['relevance']\n",
    "\n",
    "rel = []\n",
    "for i in range(0, len(score)):\n",
    "    if score[i] > 0.63 and score[i] < 0.80857142857:\n",
    "        rel.append(1)\n",
    "    if score[i] > 0.80857142857 and score[i] < 0.97714285714:\n",
    "        rel.append(1.33)\n",
    "    if score[i] > 0.97714285714 and score[i] < 1.14571428571:\n",
    "        rel.append(1.67)\n",
    "    if score[i] > 1.14571428571 and score[i] < 1.31428571428:\n",
    "        rel.append(2.01)\n",
    "    if score[i] > 1.31428571428 and score[i] < 1.48285714285:\n",
    "        rel.append(2.33)\n",
    "    if score[i] > 1.48285714285 and score[i] < 1.65142857142:\n",
    "        rel.append(2.67)\n",
    "    if score[i] > 1.65142857142 and score[i] < 1.083:\n",
    "        rel.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = df_fr['relevance']\n",
    "\n",
    "rel = []\n",
    "for i in range(0, len(score)):\n",
    "    if score[i] > 0.5 and score[i] < 1.23971428571:\n",
    "        rel.append(1)\n",
    "    if score[i] > 1.23971428571 and score[i] < 1.60542857142:\n",
    "        rel.append(1.33)\n",
    "    if score[i] > 1.60542857142 and score[i] < 1.97114285713:\n",
    "        rel.append(1.67)\n",
    "    if score[i] > 1.97114285713 and score[i] < 2.33685714284:\n",
    "        rel.append(2.01)\n",
    "    if score[i] > 2.33685714284 and score[i] < 2.70257142855:\n",
    "        rel.append(2.33)\n",
    "    if score[i] > 2.70257142855 and score[i] < 3.06828571426:\n",
    "        rel.append(2.67)\n",
    "    if score[i] > 3.06828571426 and score[i] < 3.5:\n",
    "        rel.append(3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uid = df_dt['id']\n",
    "\n",
    "sf = gl.SFrame({'id': uid, 'relevance': rel})\n",
    "\n",
    "sf.save('/Users/ssong/Downloads/IRDM/DT_submission_binned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canvas is accessible via web browser at the URL: http://localhost:62282/index.html\n",
      "Opening Canvas in default web browser.\n"
     ]
    }
   ],
   "source": [
    "df_dt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
