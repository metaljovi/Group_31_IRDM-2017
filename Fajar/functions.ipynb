{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "from spell_check_dict import spelling_checker_dict\n",
    "#stemmer = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/davida/Google Drive/IR & DM/IRDM Coursework'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check you're in the right directory guys!\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.2 |Anaconda custom (x86_64)| (default, Jul  2 2016, 17:52:12) \\n[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#YOU NEED TO DOWNLOAD THE STOPWORDS BELOW - BUT THIS MIGHT IMPROVE THE SCORE\n",
    "stop_words=nltk.corpus.stopwords.words('english')\n",
    "strNum = {'zero':0,'one':1,'two':2,'three':3,'four':4,'five':5,'six':6,'seven':7,'eight':8,'nine':9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and merging took 3.872367136005778s\n"
     ]
    }
   ],
   "source": [
    "a=timeit.default_timer()\n",
    "df_train = pd.read_csv('train.csv',encoding=\"ISO-8859-1\")\n",
    "df_test = pd.read_csv('test.csv',encoding=\"ISO-8859-1\")\n",
    "df_pro_desc = pd.read_csv('product_descriptions.csv')\n",
    "df_attr = pd.read_csv('attributes.csv')\n",
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "df_all = pd.merge(df_all, df_pro_desc, how='left', on='product_uid')\n",
    "len_train = df_train.shape[0]\n",
    "b=timeit.default_timer()\n",
    "print('loading and merging took {}s'.format(b-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed 2090 rows from df_attr\n"
     ]
    }
   ],
   "source": [
    "#check what is actually missing\n",
    "df_attr[df_attr.isnull().any(axis=1)]\n",
    "\n",
    "#we see that for some attributes, values are missing, so these attributes become useless\n",
    "\n",
    "before_len_df_attr=len(df_attr)\n",
    "df_attr.dropna(inplace=True)\n",
    "print('removed {} rows from df_attr'.format(before_len_df_attr-len(df_attr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffling train data took 0.04189527401467785s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle as sklearn_shuffle\n",
    "a=timeit.default_timer()\n",
    "df_train=sklearn_shuffle(df_train,random_state=2017)\n",
    "b=timeit.default_timer()\n",
    "print('shuffling train data took {}s'.format(b-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#PREPROCESSING PIPELINE\n",
    "def unit_converter(s):\n",
    "    try:\n",
    "        #replace units\n",
    "        s=re.sub(r\"([0-9]+)( *)(inches|inch|in|in.|')\\.?\", r\"\\1 in. \",s)\n",
    "        s=re.sub(r\"([0-9]+)( *)(pounds|pound|lbs|lb|lb.)\\.?\", r\"\\1 lb. \",s)\n",
    "        s=re.sub(r\"([0-9]+)( *)(foot|feet|ft|ft.|'')\\.?\", r\"\\1 ft. \",s)\n",
    "        s=re.sub(r\"([0-9]+)( *)(square|sq|sq.) ?\\.?(inches|inch|in|in.|')\\.?\", r\"\\1 sq.in. \",s)\n",
    "        s=re.sub(r\"([0-9]+)( *)(square|sq|sq.) ?\\.?(feet|foot|ft|ft.|'')\\.?\", r\"\\1 sq.ft. \",s)\n",
    "        s=re.sub(r\"([0-9]+)( *)(cubic|cu|cu.) ?\\.?(inches|inch|in|in.|')\\.?\", r\"\\1 cu.in. \", s)\n",
    "        s=re.sub(r\"([0-9]+)( *)(cubic|cu|cu.) ?\\.?(feet|foot|ft|ft.|'')\\.?\", r\"\\1 cu.ft. \", s)\n",
    "        s=re.sub(r\"([0-9]+)( *)(gallons|gallon|gal)\\.?\", r\"\\1 gal. \",s)\n",
    "        s=re.sub(r\"([0-9]+)( *)(ounces|ounce|oz)\\.?\", r\"\\1 oz. \",s)\n",
    "        s=re.sub(r\"([0-9]+)( *)(centimeters|cm)\\.?\", r\"\\1 cm. \",s)\n",
    "        s=re.sub(r\"([0-9]+)( *)(milimeters|mm)\\.?\", r\"\\1 mm. \",s)\n",
    "        s=re.sub(r\"([0-9]+)( *)(minutes|minute)\\.?\", r\"\\1 min. \",s)\n",
    "        s=re.sub(r\"([0-9]+)( *)(Â°|degrees|degree)\\.?\", r\"\\1 deg. \",s)\n",
    "        s=re.sub(r\"([0-9]+)( *)(v|volts|volt)\\.?\", r\"\\1 volt. \",s)\n",
    "        s=re.sub(r\"([0-9]+)( *)(wattage|watts|watt)\\.?\", r\"\\1 watt. \",s)\n",
    "        s=re.sub(r\"([0-9]+)( *)(amperes|ampere|amps|amp)\\.?\", r\"\\1 amp. \",s)\n",
    "        s=re.sub(r\"([0-9]+)( *)(qquart|quart)\\.?\", r\"\\1 qt. \",s)\n",
    "        s=re.sub(r\"([0-9]+)( *)(hours|hour|hrs.)\\.?\", r\"\\1 hr \",s)\n",
    "        s=re.sub(r\"([0-9]+)( *)(gallons per minute|gallon per minute|gal per minute|gallons/min.|gallons/min)\\.?\", \\\n",
    "         r\"\\1 gal. per min. \",s)\n",
    "        s=re.sub(r\"([0-9]+)( *)(gallons per hour|gallon per hour|gal per hour|gallons/hour|gallons/hr)\\.?\", r\"\\1 gal. per hr \",s)\n",
    "        \n",
    "        #replace things like '3x4 mm' or '3 by 4 mm' with '3 xbi 4'\n",
    "        s = s.replace(\" x \",\" xbi \")\n",
    "        s = s.replace(\"*\",\" xbi \")\n",
    "        s = s.replace(\" by \",\" xbi \")\n",
    "        s = s.replace('<b>','')\n",
    "        s = s.replace('</b>','')\n",
    "        s = s.replace('<br>','')\n",
    "        s = s.replace('</br>','')\n",
    "        #replace random weird stuff found after manual inspection\n",
    "        s = s.replace('_',' ')\n",
    "        s = s.replace('..','.')\n",
    "        s = s.replace('\\\\',' ')\n",
    "        s = s.replace('/',' ')\n",
    "        s = s.replace('  ', ' ')\n",
    "        s = s.replace('  ', ' ')\n",
    "    except:\n",
    "        raise TypeError(s,' is not a string')\n",
    "    return s\n",
    "def uppercase_splitter(s):\n",
    "    #e.g. concrete surfaceActual to concrete surface Actual\n",
    "    try:\n",
    "        s=re.sub(r\"(\\w)[\\.?!]([A-Z])\", r\"\\1 \\2\",s)\n",
    "        s=re.sub(r\"(?<=( ))([a-z]+)([A-Z]+)\", r\"\\2 \\3\",s)\n",
    "        return s\n",
    "    except:\n",
    "        raise TypeError(s, 'is not a string')\n",
    "def letter_splitter(s):\n",
    "    #handles hyphens\n",
    "    #e.g. pressure-treated to pressure treated, but keeps the the digits in place\n",
    "    #e.g. sink strainer 3-1/2 to sink strainer 3-1/2\n",
    "    try:\n",
    "        s=re.sub(r\"([a-zA-Z]+)[/\\-]([a-zA-Z]+)\", r\"\\1 \\2\",s)\n",
    "        return s\n",
    "    except:\n",
    "        raise TypeError(s, 'is not a string')\n",
    "        \n",
    "def digit_comma_merger(s):\n",
    "    #'2,000' to 2000\n",
    "    try:\n",
    "        s=re.sub(r\"(?<=\\d),(?=000)\", r\"\",s)\n",
    "        return s\n",
    "    except:\n",
    "        raise TypeError(s, 'is not a string')\n",
    "\n",
    "def digit_letter_splitter(s):\n",
    "    # 3x4x5 to 3 x 4 x 5\n",
    "    try:\n",
    "        s=re.sub(r\"(\\d+)[\\.\\-]*([a-zA-Z]+)\", r\"\\1 \\2\",s)\n",
    "        return s\n",
    "    except:\n",
    "        raise TypeError(s, 'is not a string')\n",
    "    \n",
    "def string_digit_mapper(s):\n",
    "    #HANDLES ONLY TWO DIGIT NUMBERS (not 'three hundred ten')\n",
    "    #produces gibberish otherwise\n",
    "    #twenty three to 23\n",
    "    #fourteen to 14\n",
    "    numbers={'zero': '0','one': '1','two': '2', 'three': '3','four': '4',\n",
    "             'five': '5','six': '6','seven': '7','eight': '8','nine': '9',\n",
    "             'ten': '10', 'eleven': '11', 'twelve': '12', 'thirteen': '13',\n",
    "             'fourteen': '14','fifteen': '15','sixteen': '16','seventeen': '17',\n",
    "             'eighteen': '18', 'nineteen': '19','twenty': '20','thirty': '30',\n",
    "             'forty': '40', 'fifty': '50','sixty': '60', 'seventy': '70',\n",
    "              'eighty': '80','ninety': '90'}\n",
    "    stop_numbers={'hundred':'100','thousand':'1000','million':'1000000'}\n",
    "    \n",
    "    r=s.split(' ')\n",
    "    for ind, word in enumerate(r):\n",
    "        if word in numbers:\n",
    "            try:\n",
    "                next_word=r[ind+1]\n",
    "                if next_word in numbers and next_word not in stop_numbers:\n",
    "                    try:\n",
    "                        next_next_word=r[ind+2]\n",
    "                        if next_next_word in numbers or next_next_word in stop_numbers:\n",
    "                            return s\n",
    "                        s=s.replace(word+' ','')\n",
    "                        s=s.replace(word,'')\n",
    "                        s=s.replace(next_word,numbers[word][0]+numbers[next_word])\n",
    "                        return s\n",
    "\n",
    "                    except:\n",
    "                        s=s.replace(word+' ','')\n",
    "                        s=s.replace(word,'')\n",
    "                        s=s.replace(next_word,numbers[word][0]+numbers[next_word])\n",
    "                        return s\n",
    "                \n",
    "\n",
    "            except:\n",
    "                s=s.replace(word,numbers[word])\n",
    "                return s\n",
    "    return s\n",
    "\n",
    "\n",
    "def lemmatiser(tokeniser=nltk.tokenize.TreebankWordTokenizer(),lemmatizer=nltk.stem.wordnet.WordNetLemmatizer()):\n",
    "    \n",
    "    return lambda x: \" \".join(lemmatizer.lemmatize(token) for token in tokeniser.tokenize(x) if token not in stop_words)\n",
    "\n",
    "def stemmer(tokeniser=nltk.tokenize.TreebankWordTokenizer(),lemmatizer=nltk.stem.SnowballStemmer('english')):\n",
    "    #WARNING - LEMMATISER SHOULD BE THEORETICALLY BETTER\n",
    "\n",
    "    return lambda x: \" \".join(stemmer.stem(token) for token in tokeniser.tokenize(x) if token not in stop_words)\n",
    "\n",
    "def strip_punctuation(tokeniser=nltk.tokenize.TreebankWordTokenizer()):\n",
    "    #remove all punctuation (needed for product description) except for dots\n",
    "    return lambda x: x.translate({ord(c): None for c in string.punctuation if c!='.'})\n",
    "    #return lambda x: ' '.join(token for token in tokeniser.tokenize(x) if token not in stop_words)\n",
    "\n",
    "def preprocessor():\n",
    "        #THE ORDER IS FROM BOTTOM TO TOP (i.e. starting with x.lower())\n",
    "    return lambda x:unit_converter(\\\n",
    "            string_digit_mapper(\\\n",
    "            digit_comma_merger(\\\n",
    "            digit_letter_splitter(\\\n",
    "            letter_splitter(\\\n",
    "            uppercase_splitter(\\\n",
    "            x.lower()\n",
    "                                      )\n",
    "                                          )\n",
    "                                               )\n",
    "                                                     )\n",
    "                                                          )\n",
    "                                                              )\n",
    "def spell_checker():\n",
    "        return lambda x:spelling_checker_dict[x] if x in spelling_checker_dict else x                                             \n",
    "\n",
    "tokeniser = nltk.tokenize.TreebankWordTokenizer()\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stemmer=nltk.stem.SnowballStemmer('english')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing search term took 45.55640886700712s\n",
      "preprocessing product title took 79.37594381900271s\n",
      "preprocessing product description took 542.9778857789934s\n"
     ]
    }
   ],
   "source": [
    "a=timeit.default_timer()\n",
    "#don't know why two spell checkers - just to be safe I guess?\n",
    "df_all['search_term']=df_all['search_term'].apply(spell_checker())\n",
    "df_all['search_term']=df_all['search_term'].apply(preprocessor())\n",
    "df_all['search_term']=df_all['search_term'].apply(spell_checker())\n",
    "df_all['search_term']=df_all['search_term'].apply(lemmatiser())\n",
    "b=timeit.default_timer()\n",
    "print('preprocessing search term took {}s'.format(b-a))\n",
    "\n",
    "a=timeit.default_timer()\n",
    "df_all['product_title'] = df_all['product_title'].apply(preprocessor())\n",
    "df_all['product_title'] = df_all['product_title'].apply(lemmatiser())\n",
    "b=timeit.default_timer()\n",
    "print('preprocessing product title took {}s'.format(b-a))\n",
    "\n",
    "a=timeit.default_timer()\n",
    "df_all['product_description'] = df_all['product_description'].apply(preprocessor())\n",
    "df_all['product_description'] = df_all['product_description'].apply(lemmatiser())\n",
    "b=timeit.default_timer()\n",
    "print('preprocessing product description took {}s'.format(b-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing colour took 11.472802396980114s\n",
      "preprocessing brand took 6.413068733003456s\n",
      "preprocessing size took 27.16619769501267s\n",
      "preprocessing bulletpoints took 97.45241981599247s\n"
     ]
    }
   ],
   "source": [
    "#look for colour attributes\n",
    "a=timeit.default_timer()\n",
    "\n",
    "attr_names=df_attr.name.unique().tolist()\n",
    "colour_names=[] #a list of unique colour attribute names\n",
    "for name in attr_names:\n",
    "    if type(name)==str:\n",
    "        if 'Color' in name:\n",
    "            colour_names.append(name)\n",
    "      \n",
    "            \n",
    "df_colour = df_attr[df_attr.name.isin(colour_names)][[\"product_uid\", \"value\"]]\n",
    "df_colour.columns.values[1]='product_colour'\n",
    "\n",
    "df_aggr_colour=df_colour.groupby(['product_uid'],as_index=False).aggregate(lambda x: ' '.join(list(set(x))))\n",
    "#print(df_aggr_colour['product_uid'])\n",
    "df_aggr_colour['product_colour']=df_aggr_colour['product_colour'].apply(lambda x: ' '.join(list(set(x.split()))))\n",
    "df_aggr_colour['product_colour']=df_aggr_colour['product_colour'].apply(lambda x: unit_converter(x.lower()))\n",
    "df_aggr_colour['product_colour']=df_aggr_colour['product_colour'].apply(lemmatiser())\n",
    "\n",
    "b=timeit.default_timer()\n",
    "print('preprocessing colour took {}s'.format(b-a))\n",
    "\n",
    "\n",
    "a=timeit.default_timer()\n",
    "\n",
    "df_brand = df_attr[df_attr.name == \"MFG Brand Name\"][[\"product_uid\", \"value\"]]\n",
    "df_brand.columns.values[1]='brand' #renaming\n",
    "df_brand['brand']= df_brand['brand'].apply(lemmatiser())\n",
    "\n",
    "b=timeit.default_timer()\n",
    "print('preprocessing brand took {}s'.format(b-a))\n",
    "\n",
    "\n",
    "a=timeit.default_timer()\n",
    "\n",
    "df_product_weight=df_attr[df_attr.name=='Product Weight (lb.)'][[\"product_uid\", \"value\"]]\n",
    "df_product_weight.columns.values[1]='product_weight'\n",
    "df_product_weight['product_weight']=df_product_weight['product_weight'].apply(lambda x: unit_converter(x.lower()))\n",
    "df_product_weight['product_weight']=df_product_weight['product_weight'].apply(lemmatiser())\n",
    "\n",
    "df_product_depth=df_attr[df_attr.name=='Product Depth (in.)'][[\"product_uid\", \"value\"]]\n",
    "df_product_depth.columns.values[1]='product_depth'\n",
    "\n",
    "df_product_depth['product_depth']=df_product_depth['product_depth'].apply(lambda x: unit_converter(x.lower()))\n",
    "df_product_depth['product_depth']=df_product_depth['product_depth'].apply(lemmatiser())\n",
    "\n",
    "df_product_height=df_attr[df_attr.name=='Product Height (in.)'][[\"product_uid\", \"value\"]]\n",
    "df_product_height.columns.values[1]='product_height'\n",
    "df_product_height['product_height']=df_product_height['product_height'].apply(lambda x: unit_converter(x.lower()))\n",
    "df_product_height['product_height']=df_product_height['product_height'].apply(lemmatiser())\n",
    "\n",
    "\n",
    "df_product_width=df_attr[df_attr.name=='Product Width (in.)'][[\"product_uid\", \"value\"]]\n",
    "df_product_width.columns.values[1]='product_width'\n",
    "df_product_width['product_width']=df_product_width['product_width'].apply(lambda x: unit_converter(x.lower()))\n",
    "df_product_width['product_width']=df_product_width['product_width'].apply(lemmatiser())\n",
    "\n",
    "b=timeit.default_timer()\n",
    "\n",
    "print('preprocessing size took {}s'.format(b-a))\n",
    "\n",
    "a=timeit.default_timer()\n",
    "#ONLY FIRST 5 BULLET POINTS \n",
    "df_bullets_01=df_attr[df_attr.name=='Bullet01'][['product_uid','value']]\n",
    "df_bullets_01.columns.values[1]='Bullet01'\n",
    "df_bullets_01['Bullet01']=df_bullets_01['Bullet01'].apply(lambda x: unit_converter(x.lower()))\n",
    "df_bullets_01['Bullet01']=df_bullets_01['Bullet01'].apply(lemmatiser())\n",
    "\n",
    "df_bullets_02=df_attr[df_attr.name=='Bullet02'][['product_uid','value']]\n",
    "df_bullets_02.columns.values[1]='Bullet02'\n",
    "df_bullets_02['Bullet02']=df_bullets_02['Bullet02'].apply(lambda x: unit_converter(x.lower()))\n",
    "df_bullets_02['Bullet02']=df_bullets_02['Bullet02'].apply(lemmatiser())\n",
    "\n",
    "df_bullets_03=df_attr[df_attr.name=='Bullet03'][['product_uid','value']]\n",
    "df_bullets_03.columns.values[1]='Bullet03'\n",
    "df_bullets_03['Bullet03']=df_bullets_03['Bullet03'].apply(lambda x: unit_converter(x.lower()))\n",
    "df_bullets_03['Bullet03']=df_bullets_03['Bullet03'].apply(lemmatiser())\n",
    "\n",
    "df_bullets_04=df_attr[df_attr.name=='Bullet04'][['product_uid','value']]\n",
    "df_bullets_04.columns.values[1]='Bullet04'\n",
    "df_bullets_04['Bullet04']=df_bullets_04['Bullet04'].apply(lambda x: unit_converter(x.lower()))\n",
    "df_bullets_04['Bullet04']=df_bullets_04['Bullet04'].apply(lemmatiser())\n",
    "\n",
    "df_bullets_05=df_attr[df_attr.name=='Bullet05'][['product_uid','value']]\n",
    "df_bullets_05.columns.values[1]='Bullet05'\n",
    "df_bullets_05['Bullet05']=df_bullets_05['Bullet05'].apply(lambda x: unit_converter(x.lower()))\n",
    "df_bullets_05['Bullet05']=df_bullets_05['Bullet05'].apply(lemmatiser())\n",
    "b=timeit.default_timer()\n",
    "\n",
    "print('preprocessing bulletpoints took {}s'.format(b-a))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_try = pd.merge(df_all, df_aggr_colour, how='left', on='product_uid')\n",
    "df_try = pd.merge(df_try, df_brand, how='left', on='product_uid')\n",
    "df_try = pd.merge(df_try, df_product_weight, how='left', on='product_uid')\n",
    "df_try = pd.merge(df_try, df_product_depth, how='left', on='product_uid')\n",
    "df_try = pd.merge(df_try, df_product_height, how='left', on='product_uid')\n",
    "df_try = pd.merge(df_try, df_product_width, how='left', on='product_uid')\n",
    "df_try = pd.merge(df_try, df_bullets_01, how='left', on='product_uid')\n",
    "df_try = pd.merge(df_try, df_bullets_02, how='left', on='product_uid')\n",
    "df_try = pd.merge(df_try, df_bullets_03, how='left', on='product_uid')\n",
    "df_try = pd.merge(df_try, df_bullets_04, how='left', on='product_uid')\n",
    "df_try = pd.merge(df_try, df_bullets_05, how='left', on='product_uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_all=df_try\n",
    "df_all=pd.read_csv('raw.csv')\n",
    "df_all=df_all[df_all.search_term.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Levenshtein'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9450b38ea610>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdifflib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequenceMatcher\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mLevenshtein\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msequence_match_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Levenshtein'"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "import Levenshtein\n",
    "import distance\n",
    "\n",
    "def sequence_match_score(s1,s2):\n",
    "    #Ratcliff-Obershelp algorithm score\n",
    "    seq=SequenceMatcher(None,s1,s2)\n",
    "    \n",
    "    return seq.ratio()\n",
    "\n",
    "def levenshtein_dist(s1,s2):\n",
    "    #normalized Levenshtein distance\n",
    "    \n",
    "    return Levenshtein.ratio(s1,s2)\n",
    "\n",
    "def Jaccard_dist(s1,s2):\n",
    "    #normalized Jaccardian distance\n",
    "    set1, set2= set(s1.split()),set(s2.split())\n",
    "    \n",
    "    return float(len(set1 & set2)) / len(set1 | set2)\n",
    "\n",
    "def full_string_match_count(s1, s2):\n",
    "    \n",
    "    #count the number of times s1 appears in s2\n",
    "    count = 0\n",
    "    index = 0\n",
    "    while index < len(s2):\n",
    "        index = s2.find(s1, index)\n",
    "        if index == -1:\n",
    "            #no match\n",
    "            return count\n",
    "        else:\n",
    "            count += 1\n",
    "            index += len(s1)\n",
    "    return count\n",
    "\n",
    "def common_words_match_count(s1, s2):\n",
    "    \n",
    "    #count the number of common words between s1 and s2\n",
    "    return sum([int(s2.find(word)>=0) for word in s1.split()])\n",
    "\n",
    "test_string=['hello world','hello world hello. My name is hello world']\n",
    "print(full_string_match_count(test_string[0],test_string[1]))\n",
    "\n",
    "print(common_words_match_count(test_string[0],test_string[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all['num_words_query'] = df_all['search_term'].map(lambda x:len(x.split()))\n",
    "df_all['num_words_title'] = df_all['product_title'].map(lambda x:len(x.split()))\n",
    "df_all['num_words_brand'] = df_all['brand'].map(lambda x:len(x.split()) if type(x)==str else 1)\n",
    "df_all['num_words_colour'] = df_all['product_colour'].map(lambda x:len(x.split()) if type(x)==str else 1)\n",
    "df_all['num_words_weight'] = df_all['product_weight'].map(lambda x:len(x.split()) if type(x)==str else 1)\n",
    "df_all['num_words_height'] = df_all['product_height'].map(lambda x:len(x.split()) if type(x)==str else 1)\n",
    "df_all['num_words_depth'] = df_all['product_depth'].map(lambda x:len(x.split()) if type(x)==str else 1 )\n",
    "df_all['num_words_width'] = df_all['product_width'].map(lambda x:len(x.split()) if type(x)==str else 1 )\n",
    "\n",
    "\n",
    "\n",
    "#SEARCH TERM AND PRODUCT TITLE SECTION\n",
    "a=timeit.default_timer()\n",
    "\n",
    "df_query_and_title=df_all['search_term'] + '||' + df_all['product_title']\n",
    "\n",
    "df_all['query_in_title']=df_query_and_title.map(lambda x: full_string_match_count(\n",
    "    x.split('||')[0],x.split('||')[1]))\n",
    "df_all['common_words_query_and_title']=df_query_and_title.map(lambda x: common_words_match_count(\n",
    "    x.split('||')[0],x.split('||')[1]))\n",
    "df_all['query_last_word_in_title'] = df_query_and_title.map(lambda x:common_words_match_count(x.split('||')[0].split(\" \")[-1],\n",
    "                                                                                              x.split('||')[1]))\n",
    "\n",
    "df_all['title_seq_match_score']=df_query_and_title.map(lambda x: sequence_match_score(x.split('||')[0],\n",
    "                                                                                      x.split('||')[1]))\n",
    "\n",
    "df_all['title_levenshtein_ratio']=df_query_and_title.map(lambda x: levenshtein_dist(x.split('||')[0],\n",
    "                                                                                    x.split('||')[1]))\n",
    "\n",
    "df_all['title_Jaccard_dist_norm']=df_query_and_title.map(lambda x: Jaccard_dist(x.split('||')[0],\n",
    "                                                                                x.split('||')[1]))\n",
    "df_all['ratio_title'] = df_all['common_words_query_and_title']/df_all['num_words_query']\n",
    "\n",
    "del df_query_and_title\n",
    "b=timeit.default_timer()\n",
    "\n",
    "print('product title feature engineering took {}s'.format(b-a))\n",
    "\n",
    "#SEARCH TERM AND DESCRIPTION\n",
    "a=timeit.default_timer()\n",
    "\n",
    "df_query_and_desc = df_all['search_term'] + '||' + df_all['product_description']\n",
    "df_all['query_in_description'] = df_query_and_desc.map(lambda x: full_string_match_count(x.split('||')[0],\n",
    "                                                                                         x.split('||')[1]))\n",
    "\n",
    "df_all['common_words_query_and_desc']= df_query_and_desc.map(lambda x: common_words_match_count(\n",
    "    x.split('||')[0],x.split('||')[1]))\n",
    "df_all['query_last_word_in_desc'] = df_query_and_desc.map(lambda x:common_words_match_count(x.split('||')[0].split(\" \")[-1],\n",
    "                                                                                            x.split('||')[1]))\n",
    "\n",
    "df_all['desc_sequence_match_score'] = df_query_and_desc.map(lambda x: sequence_match_score(x.split('||')[0],\n",
    "                                                                                      x.split('||')[1]))\n",
    "df_all['desc_levenshtein_ratio'] = df_query_and_desc.map(lambda x: levenshtein_dist(x.split('||')[0],\n",
    "                                                                                x.split('||')[1]))\n",
    "df_all['desc_Jaccard_dist_norm'] = df_query_and_desc.map(lambda x: Jaccard_dist(x.split('||')[0],\n",
    "                                                                                x.split('||')[1]))\n",
    "\n",
    "df_all['ratio_description'] = df_all['common_words_query_and_desc'] / df_all['num_words_query']\n",
    "\n",
    "del df_query_and_desc \n",
    "b=timeit.default_timer()\n",
    "\n",
    "print('product desc feature engineering took {}s'.format(b-a))\n",
    "\n",
    "#SEARCH TERM AND BRAND\n",
    "a=timeit.default_timer()\n",
    "\n",
    "df_query_and_brand = df_all['search_term'] + '||' + df_all['brand']\n",
    "df_query_and_brand.dropna(inplace=True)\n",
    "df_all['query_in_brand'] = df_query_and_brand.map(lambda x: full_string_match_count(x.split('||')[0],\n",
    "                                                                                    x.split('||')[1]))\n",
    "\n",
    "df_all['common_words_query_and_brand']= df_query_and_brand.map(lambda x: common_words_match_count(x.split('||')[0],\n",
    "    x.split('||')[1]))\n",
    "#last word in brand doesn't make sense\n",
    "\n",
    "df_all['brand_sequence_match_score'] = df_query_and_brand.map(lambda x: sequence_match_score(x.split('||')[0],\n",
    "                                                                                      x.split('||')[1]))\n",
    "df_all['brand_levenshtein_ratio'] = df_query_and_brand.map(lambda x: levenshtein_dist(x.split('||')[0],\n",
    "                                                                                x.split('||')[1]))\n",
    "df_all['brand_Jaccard_dist_norm'] = df_query_and_brand.map(lambda x: Jaccard_dist(x.split('||')[0],\n",
    "                                                                                x.split('||')[1]))\n",
    "\n",
    "df_all['ratio_brand'] = df_all['common_words_query_and_brand'] / df_all['num_words_query']\n",
    "\n",
    "del df_query_and_brand\n",
    "b=timeit.default_timer()\n",
    "\n",
    "print('brand feature engineering took {}s'.format(b-a))\n",
    "\n",
    "#SEARCH TERM AND COLOUR\n",
    "\n",
    "a=timeit.default_timer()\n",
    "\n",
    "df_query_and_colour = df_all['search_term']+'||'+df_all['product_colour']\n",
    "df_query_and_colour.dropna(inplace=True) #dropna cause brands do not exist for every product uid and hence a search term\n",
    "df_all['query_in_colour'] = df_query_and_colour.map(lambda x: full_string_match_count(x.split('||')[0],\n",
    "                                                                                    x.split('||')[1]))\n",
    "\n",
    "df_all['common_words_query_and_colour']= df_query_and_colour.map(lambda x: common_words_match_count(x.split('||')[0],\n",
    "    x.split('||')[1]))\n",
    "#last word in colour doesn't make sense\n",
    "df_all['query_first_word_in_colour'] = df_query_and_colour.map(lambda x:common_words_match_count(x.split('||')[0].split(\" \")[0],\n",
    "                                                                                            x.split('||')[1]))\n",
    "\n",
    "df_all['colour_sequence_match_score'] = df_query_and_colour.map(lambda x: sequence_match_score(x.split('||')[0],\n",
    "                                                                                      x.split('||')[1]))\n",
    "df_all['colour_levenshtein_ratio'] = df_query_and_colour.map(lambda x: levenshtein_dist(x.split('||')[0],\n",
    "                                                                                x.split('||')[1]))\n",
    "df_all['colour_Jaccard_dist_norm'] = df_query_and_colour.map(lambda x: Jaccard_dist(x.split('||')[0],\n",
    "                                                                                x.split('||')[1]))\n",
    "\n",
    "df_all['ratio_colour'] = df_all['common_words_query_and_colour'] / df_all['num_words_query']\n",
    "\n",
    "del df_query_and_colour\n",
    "\n",
    "\n",
    "b=timeit.default_timer()\n",
    "\n",
    "print('colour feature engineering took {}s'.format(b-a))\n",
    "\n",
    "\n",
    "#SEARCH TERM AND SIZE \n",
    "\n",
    "a=timeit.default_timer()\n",
    "df_query_and_size = df_all['search_term'] + '||' + df_all['product_weight'] + ' ' +\\\n",
    "df_all['product_height'] + ' ' + df_all['product_depth'] + ' ' + df_all['product_width']\n",
    "\n",
    "df_query_and_size.dropna(inplace=True) \n",
    "\n",
    "#query in size doesn't make sense\n",
    "df_all['common_words_query_and_size']= df_query_and_size.map(lambda x: common_words_match_count(x.split('||')[0],\n",
    "    x.split('||')[1]))\n",
    "\n",
    "#first word in size doesn't make sense\n",
    "df_all['query_last_word_in_size'] = df_query_and_size.map(lambda x:common_words_match_count(x.split('||')[0].split(\" \")[-1],\n",
    "                                                                                            x.split('||')[1]))\n",
    "\n",
    "df_all['size_sequence_match_score'] = df_query_and_size.map(lambda x: sequence_match_score(x.split('||')[0],\n",
    "                                                                                      x.split('||')[1]))\n",
    "df_all['size_levenshtein_ratio'] = df_query_and_size.map(lambda x: levenshtein_dist(x.split('||')[0],\n",
    "                                                                                x.split('||')[1]))\n",
    "df_all['size_Jaccard_dist_norm'] = df_query_and_size.map(lambda x: Jaccard_dist(x.split('||')[0],\n",
    "                                                                                x.split('||')[1]))\n",
    "\n",
    "df_all['ratio_size'] = df_all['common_words_query_and_size'] / df_all['num_words_query']\n",
    "\n",
    "del df_query_and_size\n",
    "\n",
    "b=timeit.default_timer()\n",
    "\n",
    "print('size feature engineering took {}s'.format(b-a))\n",
    "\n",
    "#SEARCH TERM AND BULLETPOINTS\n",
    "a=timeit.default_timer()\n",
    "\n",
    "df_query_and_bullets = df_all['search_term'] + '||'  + df_all['Bullet01'] + df_all['Bullet02'] + ' ' +\\\n",
    "df_all['Bullet03'] + ' ' + df_all['Bullet04'] + ' ' + df_all['Bullet05']\n",
    "\n",
    "df_query_and_bullets.dropna(inplace=True) \n",
    "\n",
    "df_all['query_in_bullets'] = df_query_and_bullets.map(lambda x: full_string_match_count(x.split('||')[0],\n",
    "                                                                                    x.split('||')[1]))\n",
    "\n",
    "df_all['common_words_query_and_bullets']= df_query_and_bullets.map(lambda x: common_words_match_count(x.split('||')[0],\n",
    "    x.split('||')[1]))\n",
    "\n",
    "#last word in bulletpoints doesn't make sense\n",
    "df_all['bullets_sequence_match_score'] = df_query_and_bullets.map(lambda x: sequence_match_score(x.split('||')[0],\n",
    "                                                                                      x.split('||')[1]))\n",
    "df_all['bullets_levenshtein_ratio'] = df_query_and_bullets.map(lambda x: levenshtein_dist(x.split('||')[0],\n",
    "                                                                                x.split('||')[1]))\n",
    "df_all['bullets_Jaccard_dist_norm'] = df_query_and_bullets.map(lambda x: Jaccard_dist(x.split('||')[0],\n",
    "                                                                                x.split('||')[1]))\n",
    "\n",
    "df_all['ratio_bullets'] = df_all['common_words_query_and_bullets'] / df_all['num_words_query']\n",
    "\n",
    "del df_query_and_bullets\n",
    "\n",
    "b=timeit.default_timer()\n",
    "\n",
    "print('bulletpoints feature engineering took {}s'.format(b-a))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all.to_csv('ready.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
